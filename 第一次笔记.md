

## Encoder-Decoder
### 概述Encoder-Decoder
Encoder-Decoder是一类算法，是一种模型架构，被广泛用于机器翻译、语音识别等任务。

**Encoder**
Encoder 又称作编码器。它的作用就是“将现实问题转化为数学问题”
![[learning/sca/paper-reading/attachments/encoder编码器概念解释示意图.png]]

**Decoder**
Decoder 又称作解码器，他的作用是“求解数学问题，并转化为现实世界的解决方案”

![[learning/sca/paper-reading/attachments/decoder解码器概念解释示意图.png]]

把 2 个环节连接起来，用通用的图来表达则是下面的样子：

![[learning/sca/paper-reading/attachments/encdoer-decoder概念解释示意图.png]]

对于一个Encoder-Decoder模型而言：
>1.  不论输入和输出的长度是什么，中间的“向量 c” 长度都是固定的（这也是它的缺陷）
>2.  根据不同的任务可以选择不同的编码器和解码器（可以是一个 _[RNN](https://easyai.tech/ai-definition/rnn/)_ ，但通常是其变种 _[LSTM](https://easyai.tech/ai-definition/lstm/)_ 或者 _GRU_ ）




### transformer

Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。

一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入：
![[learning/sca/paper-reading/attachments/Transformer 6层叠加示意图.png]]


![[learning/sca/paper-reading/attachments/Transformer每个encoder-decoder基本单元构造示意图.png]]

>1.  Self-Attention：当前翻译和已翻译的前文之间的关系；
>2.  Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。
>3.  Feed Forward Neural Network，这个全连接有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为：$FFN(Z)=max(0,ZW_1+b_1)W_2+b_2$

transform的整体结构如下图所示；在解码器中，Transformer block比编码器中多了个encoder-decoder attention。
在encoder-decoder attention中， Q 来自于解码器的上一个输出，K  和 V 则来自于与编码器的输出。
由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 K 个特征向量时，我们只能看到第 K-1 及其之前的解码结果，论文中把这种情况下的multihead attention叫做masked multi-head attention。transfrom的encoder和decoder整体架构如下：

![[learning/sca/paper-reading/attachments/transfrom的encoder和decoder整体架构.png]]

tranformer不再使用NLP中最根本的RNN或者CNN设计上具有很大创新，理论上所有用上RNN、CNN的地方都可以用transformer作替换尝试。但是transformer一九没有逃脱传统学习的套路，本质上也是一个全连接不过加上了一个attention机制，但是简单的抛弃RNN和CNN也会让模型丧失捕捉局部特征的能力，可以进行组合使用。


## InternLM2-7B-Chat
